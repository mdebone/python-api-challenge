# python-api-challenge

I had some initial problems with using API keys, hiding them in a py file, but thats not secure, adding them to other . files but they wouldn't stick for whatever reason and I would always get the 400 errors, no key found so finally got this to work and was able to pull the weather data. I tried doing something I saw from a developer on a youtube video on adding a wait period after 49 extractions, as the cap on call rate is 60 requests per minute, it would add a time wait element but it didn't work well, I think it may have been nested wrong because I was always receiving no city found, moving on to next request at a pretty quick rate so I don't think it was nested correctly. I also tried and a 1.01 second time element per request, which seemed pretty clever, if the ceiling is 60 requests per minute, you're doing probably 59 requests for however many cities you need and would never hit the ceiling. I didn't get it to work, but then agian the data that I pulled on cities was a list of 625 with about 50-75 null/na/nan cities. This was about after 10-15 attempts at pulling the cities with key errors and then there were binning errors for the dictionaries that I had set up to recieve the data. 

Im pretty certain that it was not an issue with my schema of finding the route to the various items and where they were nested, like weather stuff is under temp, location stuff is under main, all of that was correct. When I say binning its because I wasn't just setting up lists to hold the latitudes and longitudes, I thought that I could take the list of the random cities I found and print it to the first column of a dataframe that would be city name, the index would be the count and the rest would be found and sorted automatically under their appropriately column header which just had a "" in there and my output wouldn't be a dictionary of lists but a dataframe, skipping a step and saving myself some time. Easy peasey.

Ohh the hubris of man. 

It didn't work, and ohh did I waist many a man-hour of attempting my very own, self-imposed, sisyphean struggle. After cutting my losses and realizing that the royal 'we' weren't making any progress, I did it as suggested, make a bunch of lists, put the data into those lists, and then after extraction append them all to a database. But I will note that even after I attempted to get that working as instruced my query url wasn't working for whatever reason. I had it set up with {imperial units}, {api key}, and {city} (because it was pulling the city name from the database I had so cleverly constructed but that wasn't working. Dropping the city request of the query url and just seeing what it returned didn't work either, I had  to requests the url, which I don't know why I was adverse to but it wasn't part of the instructions so it seemed like it was going down another road that I didn't want to go down but it actually turned out to make the request work, on the n-th attempted call. Once I had my sacred csv I made many copies and stashed it in like 5 different places, I wasn't going to lose my precious data. from there it was pretty straightforward, the only thing that threw me was at the end for the 8 charts on the params of the northern and southern hemispheres. I thought that I could just use the database and make the call directtly with the xlim of 0-90 for north and -90-0 for south. And while it was charting the scatter data for the correct hemisphere, the line of central tendency was still being written with the all of the temperature data for example, both north and south. Like if you held them side by side, the southern hemisphere line would trend down pass thru the equator and connect to the line of the graph on the right, say of the northern hemisphere. After realising that, and realising that the key here was litteraly latitude graphs, I just took the original output db, created two new ones, north and south, made sure that therer werent any cities that I lost on the equator, there weren't, my total was correct, but even if there had been I would have just done one filter function as < and one as >=. From there it was just everything repeated and pretty simple. I didn't declare the path of the png files output to the charts folder, to the best of my knowledge you can't do that when you are creating the graph with plyplot, I tried a few iterations, but they're all in there. Done purley as a housekeeping measure. 

Got the vacation data to update with the heat map, the first humidity heat map test ran fine, had a lot of locations and then I went about filtering. I chose temperature < 85 and >32 which got rid of 60 locations, cloudiness and humidity < 80 % seemed reasonable enought although I was also hesitant on those two specifications because I realized that, longitudinally speaking, the sampling of one random day of weather that had rain was probably not representative of situations like overcast grey rust-belt-ly cities but might actually represent places in the tropics that get a light shower everyday but does not represent an undesireable location to visit. Would likely take out half the Cariabean islands with that filtering, but I was also cognizant that it was a hypothetical exercise and I was probably over analyzing it too much. So they went. It also helped to prune the 550 locations down to 150 given the number of calls that I was going to have to do on hotels.

That was where the difficulty came in, the api pull. I knew that I had to utilize the lat and lng as specifications, 'hotels' was the key filter by but going thru the Places Google API documentation I knew that 'Find Place' in the Dev documentation on the website was probably what I wanted, and that was what we had done a lot of examples from class regarding so I went with it first. I knew that the my type was 'lodging' as there is no type for 'hotels' so it wasn't that, but all the other specifications where correct, 5000 m lat, lng and I even tried switching it up again to the Nearby Search, careful not to type hotel and hotel as a double parameters cause apparently that causes errors (see, I did read the documentation thuroughly) and actually winds up returning no results. Whatever the case, I read a Towards Data Science article about using Text Search, the third and least touched on specification because what I want is not the RitzCarlton or Ceasar's Palace but I am ambiguiously asking for hotels in locations many of which I had not heard of, that got it to work. They used query in the above mentioned article to find coffee shops, and it made a lot of sense, I am generally asking for hotels. So I set it to run and this time in pinged back a lot of data rather than what it had been doing before of itterating thru 150 rows with nothing found. Success! it printed to the dataframe column and I was in business. 
